{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Analytics Software\\Anaconda\\anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Analytics Software\\Anaconda\\anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation, metrics, linear_model, ensemble, tree, grid_search, externals\n",
    "from time import time\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import externals, metrics\n",
    "import cPickle as pickle\n",
    "from __future__ import division\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benli\\Documents\\Project\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\benli\\Documents\\Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_train = pd.read_pickle('reviews_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the dataset only for restaurants\n",
    "reviews_train = reviews_train[reviews_train['category'] == 'Restaurants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the dataset only for Nevada state\n",
    "reviews_train = reviews_train[reviews_train['state'] == 'NV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean reviews by keeping only alphabetic characters and exclamation marks\n",
    "text_cleaned = [re.sub(r'!', ' exclamation', text) for text in reviews_train['text']]\n",
    "text_cleaned = [text.lower() for text in text_cleaned]\n",
    "text_cleaned = [re.sub(r'[^a-z]', ' ', text) for text in text_cleaned]\n",
    "reviews_train['text_cleaned'] = text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(505807, 10290)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create document-term matrix using the cleaned reviews\n",
    "vectorizer = CountVectorizer(min_df = 100)\n",
    "dtm = vectorizer.fit_transform(reviews_train['text_cleaned'])\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate sentiment scores, first calculate the percentage of time a word \"spends\" on each document, defined as its usage in each individual document divided by its total usage across all documents (i.e., the column sum), thus creating a modified document-term matrix whose elements are all \"percentage frequencies.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the total frequency across all documents for each word (i.e., the column sums)\n",
    "freq = csr_matrix.sum(dtm, 0)\n",
    "\n",
    "# devide the individual frequency by the total frequency to calculate the percentage of time a word \"spends\" on each document\n",
    "freq_inverse = np.repeat(1, freq.shape[1]) / freq\n",
    "freq_inverse = csr_matrix(freq_inverse)\n",
    "dtm_prc = dtm.multiply(freq_inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each word, multiply its percentage frequency in each document by the number of stars the reviewer ultimately assigned to the business (normalized to [-1, 1]), and sum these weighted frequencies across all documents to arrive at the word's sentiment score.\n",
    "\n",
    "As an example, say the word \"good\" shows up 5 times in a 5-star review, 4 times in a 4-star review, ..., and 1 time in a 1-star review, its sentiment score would be (5 * 1 + 4 * 0.5 + 3 * 0 + 2 * (-0.5) + 1 * (-1)) / (5 + 4 + 3 + 2 + 1) = 0.33, indicating moderately postive sentiment.\n",
    "\n",
    "As a couple extreme examples, if a word only appears in 5-star reviews, its sentiment score would be 1, and if it only appears in 1-star reviews, its score would be -1.\n",
    "\n",
    "What about highly frequent words that don't mean much (e.g., stopwords)? As shown in the first example, showing up everywhere will dilute a word's overall effect. In an extreme example, if a word's usage is evenly distributed across all documents, its final score would be 0, indicating abosulately neutral effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xc4bdc358>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the sentiment score for each word by weighting its percentage frequency by the number of stars\n",
    "# first normalize the star ratings to [-1, 1]:\n",
    "def normalization(series):\n",
    "    return (2 * series - (series.max() + series.min())) / (series.max() - series.min())\n",
    "stars_norm = normalization(reviews_train['stars'])\n",
    "\n",
    "# calculate word scores using the normlized star ratings\n",
    "word_score = dtm_prc.T * stars_norm\n",
    "sns.distplot(word_score, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine with the terms\n",
    "word2score = dict(zip(vectorizer.get_feature_names(), word_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate sentiment scores for each reviews\n",
    "review_score = dtm * np.atleast_2d(word_score).T\n",
    "\n",
    "# divide the scores by the total number of words in each document\n",
    "dtm_row_sum = csr_matrix.sum(dtm, 1)\n",
    "review_score = review_score / dtm_row_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of reviews with NA scores: 94\n"
     ]
    }
   ],
   "source": [
    "# for reviews that don't include any words in the dtm, replace the 0 scores with NAs\n",
    "review_score[dtm_row_sum == 0] = np.nan\n",
    "print 'the number of reviews with NA scores:', sum(pd.isnull(review_score))[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine scores with the reviews\n",
    "reviews_train['score'] = review_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing na\n",
    "reviews_train_excl_na = reviews_train.dropna(subset = ['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create input and output variables\n",
    "score = reviews_train_excl_na['score']\n",
    "score = np.atleast_2d(score).T # the feature set needs to be 2-dimensional\n",
    "stars = reviews_train_excl_na['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scorer_mae = metrics.make_scorer(metrics.mean_absolute_error, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family:Times New Roman;\">Logistic Regression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_clf = linear_model.LogisticRegression(random_state = 2014)\n",
    "\n",
    "t0 = time()\n",
    "logit_scores_mae = cross_validation.cross_val_score(logit_clf, score, stars, cv = 10, scoring = scorer_mae)\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression using mean-absolute-error scoring function\n",
      "time used: 18 seconds\n",
      "scores: -0.712 (+/-0.003)\n"
     ]
    }
   ],
   "source": [
    "print 'logistic regression using mean-absolute-error scoring function'\n",
    "print 'time used: %d seconds' % (t1 - t0)\n",
    "print 'scores: %0.3f (+/-%0.03f)' % (logit_scores_mae.mean(), logit_scores_mae.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic model finished quickly with decent results, indicating the computed sentiment scores can linearly separate the data to some extent. The mean scores of the 10-fold cross-validation is 0.712 (the negative sign is added automatically indicating the new scoring function is a loss function), suggesting on average the predicted ratings are within 1-star away from the actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family:Times New Roman;\">Baggigng Trees</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_clf = ensemble.BaggingClassifier(tree.DecisionTreeClassifier(), n_estimators = 100, n_jobs = -1, random_state = 2014)\n",
    "\n",
    "t0 = time()\n",
    "bag_scores_mae = cross_validation.cross_val_score(bag_clf, score, stars, cv = 10, scoring = scorer_mae)\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging trees using mean-absolute-error scoring function\n",
      "time used: 1257 seconds\n",
      "scores: -0.783 (+/-0.004)\n"
     ]
    }
   ],
   "source": [
    "print 'bagging trees using mean-absolute-error scoring function'\n",
    "print 'time used: %d seconds' % (t1 - t0)\n",
    "print 'scores: %0.3f (+/-%0.03f)' % (bag_scores_mae.mean(), bag_scores_mae.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family:Times New Roman;\">Gradient boost trees</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient boost trees\n",
    "param_grid = {'learning_rate': [.01, .1], 'max_depth': [1, 5]}\n",
    "gbm_clf = grid_search.GridSearchCV(ensemble.GradientBoostingClassifier(n_estimators = 100, random_state = 2014),\n",
    "                                   param_grid, cv = 10, scoring = scorer_mae, n_jobs = -1)\n",
    "\n",
    "t0 = time()\n",
    "gbm_clf.fit(score, stars)\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient boost trees using mean-absolute-error scoring function\n",
      "time used: 3241 seconds\n",
      "best score: -0.594242188751 {'learning_rate': 0.1, 'max_depth': 1}\n",
      "grid scores:\n",
      "scores: -0.627 (+/-0.004) {'learning_rate': 0.01, 'max_depth': 1}\n",
      "scores: -0.595 (+/-0.003) {'learning_rate': 0.01, 'max_depth': 5}\n",
      "scores: -0.594 (+/-0.003) {'learning_rate': 0.1, 'max_depth': 1}\n",
      "scores: -0.595 (+/-0.003) {'learning_rate': 0.1, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "print 'gradient boost trees using mean-absolute-error scoring function'\n",
    "print 'time used: %d seconds' % (t1 - t0)\n",
    "print 'best score:', gbm_clf.best_score_, gbm_clf.best_params_\n",
    "print 'grid scores:'\n",
    "for scores in gbm_clf.grid_scores_:\n",
    "    print 'scores: %0.3f (+/-%0.03f)' % (scores[2].mean(), scores[2].std()), scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family:Times New Roman;\">Saving the winning model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gbm_clf.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the winning model\n",
    "# Please save the model with the highest accuracy\n",
    "externals.joblib.dump(gbm_clf, 'gbm_clf.pkl')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
